# mypy: ignore-errors
"""Pass to John PM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FlaOARH_bgU1711c_iiCGBzk1s8hMjLj
"""


from google.colab import drive
import json
from traitlets.traitlets import Int
import pandas as pd

# import numpy as nm

drive.mount("/content/drive")

# Import and Merge Dataframes
GDP = pd.read_csv(
    "/content/drive/Shareddrives/EB-198 Research/Data/output/economic/GDP/gdp_current_dollars.csv", index_col=False
)
UI = pd.read_csv(
    "/content/drive/Shareddrives/EB-198 Research/Data/output/economic/UI/ar203_output.csv", index_col=False
)
Cons = pd.read_csv(
    "/content/drive/Shareddrives/EB-198 Research/Data/output/economic/consumption/consumption.csv", index_col=False
)
dCons = pd.read_csv(
    "/content/drive/Shareddrives/EB-198"
    " Research/Data/output/economic/consumption_durable_goods/consumption_durable_goods.csv",
    index_col=False,
)
ndCons = pd.read_csv(
    "/content/drive/Shareddrives/EB-198"
    " Research/Data/output/economic/consumption_nondurable_goods/consumption_nondurable_goods.csv",
    index_col=False,
)
serCons = pd.read_csv(
    "/content/drive/Shareddrives/EB-198 Research/Data/output/economic/consumption_services/consumption_services.csv",
    index_col=False,
)
tgCons = pd.read_csv(
    "/content/drive/Shareddrives/EB-198"
    " Research/Data/output/economic/consumption_total_goods/consumption_total_goods.csv",
    index_col=False,
)
Employ1 = pd.read_csv(
    "/content/drive/Shareddrives/EB-198 Research/Data/output/economic/employment/CE16OV_output.csv", index_col=False
)
Employ2 = pd.read_csv(
    "/content/drive/Shareddrives/EB-198 Research/Data/output/economic/employment/PAYEMS_output.csv", index_col=False
)
CPIu = pd.read_csv(
    "/content/drive/Shareddrives/EB-198 Research/Data/output/economic/inflation/CPIAUCSL_output.csv", index_col=False
)
corePCE = pd.read_csv(
    "/content/drive/Shareddrives/EB-198 Research/Data/output/economic/inflation/PCEPILFE_output.csv", index_col=False
)
PCE = pd.read_csv(
    "/content/drive/Shareddrives/EB-198 Research/Data/output/economic/inflation/PCEPI_output.csv", index_col=False
)
Int = pd.read_csv(
    "/content/drive/Shareddrives/EB-198 Research/Data/output/economic/interest_rates/rates_output.csv", index_col=False
)
Assets = pd.read_csv(
    "/content/drive/Shareddrives/EB-198 Research/Data/output/economic/investment/K1TTOTL1ES000_output.csv",
    index_col=False,
)
RPFI = pd.read_csv(
    "/content/drive/Shareddrives/EB-198 Research/Data/output/economic/investment/RPFI_output.csv", index_col=False
)
Prod = pd.read_csv(
    "/content/drive/Shareddrives/EB-198"
    " Research/Data/output/economic/productivity/labor-productivity-by-state-and-region_output.csv",
    index_col=False,
)
Unemp = pd.read_csv(
    "/content/drive/Shareddrives/EB-198 Research/Data/output/economic/unemployment/ststdsadata_output.csv",
    index_col=False,
)
ARP = pd.read_csv(
    "/content/drive/Shareddrives/EB-198"
    " Research/Data/output/intervention/american_rescue_plan/fiscalrecoveryfunds-statefunding1-CSV_output.csv",
    index_col=False,
)
CARES = pd.read_csv(
    "/content/drive/Shareddrives/EB-198 Research/Data/output/intervention/cares/cares_state_allocation_output.csv",
    index_col=False,
)
PPP = pd.read_csv(
    "/content/drive/Shareddrives/EB-198 Research/Data/output/intervention/ppp/PPP_states_and_territories_output.csv",
    index_col=False,
)
CDCcasesdeaths = pd.read_csv(
    "/content/drive/Shareddrives/EB-198 Research/Data/output/pandemic/cases/CDC_state_cases_and_deaths_output.csv",
    index_col=False,
)
JHUcases = pd.read_csv(
    "/content/drive/Shareddrives/EB-198 Research/Data/output/pandemic/cases/JHU_time_series_cases_output.csv",
    index_col=False,
)
moredeaths = pd.read_csv(
    "/content/drive/Shareddrives/EB-198 Research/Data/output/pandemic/deaths/time_series_covid19_deaths_US_output_FIXED"
    " - time_series_covid19_deaths_US_output.csv",
    index_col=False,
)
Vax1 = pd.read_csv(
    "/content/drive/Shareddrives/EB-198"
    " Research/Data/output/pandemic/vaccinations/19_Vaccinations_in_the_United_States_Jurisdiction_output_output.csv",
    index_col=False,
)
Vax2 = pd.read_csv(
    "/content/drive/Shareddrives/EB-198"
    " Research/Data/output/pandemic/vaccinations/time_series_covid19_vaccine_us_output.csv",
    index_col=False,
)
Distance = pd.read_csv(
    "/content/drive/Shareddrives/EB-198"
    " Research/Data/output/state_mandates/UWashington/USstatesCov19distancingpolicyBETA_complete_output.csv",
    index_col=False,
)
Mask = pd.read_csv(
    "/content/drive/Shareddrives/EB-198"
    " Research/Data/output/state_mandates/mask_mandates/U.S._State_and_Territorial_Public_Mask_Mandates_From_April_8__2020_through_August_15__2021_by_State_by_Day_output.csv",
    index_col=False,
)
CPI1 = pd.read_csv(
    "/content/drive/Shareddrives/EB-198 Research/Data/output/economic/inflation/pcpiMvMd.csv", index_col=False
)
Fedfunds = pd.read_csv(
    "/content/drive/Shareddrives/EB-198 Research/Data/output/economic/interest_rates/FEDFUNDS_output.csv",
    index_col=False,
)
sah = pd.read_csv(
    "/content/drive/Shareddrives/EB-198"
    " Research/Data/output/state_mandates/shelter_in_place/U.S._State_and_Territorial_Stay-At-Home_Orders__March_15__2020___August_15__2021_by_County_by_Day_output.csv",
    index_col=False,
)
cdcsurv = pd.read_csv(
    "/content/drive/Shareddrives/EB-198 Research/Data/output/pandemic/cases/CDC_Case_Surveillance.csv", index_col=False
)
# print(cdcsurv.loc[(cdcsurv['State'] == 'AK') & (cdcsurv['Year'] == 2020)])

df = (
    GDP.merge(UI, on=["State", "Year", "Period"])
    .merge(Cons, on=["State", "Year", "Period"])
    .merge(dCons, on=["State", "Year", "Period"])
    .merge(ndCons, on=["State", "Year", "Period"])
    .merge(serCons, on=["State", "Year", "Period"])
    .merge(tgCons, on=["State", "Year", "Period"])
    .merge(Employ1, on=["State", "Year", "Period"])
    .merge(Employ2, on=["State", "Year", "Period"])
    .merge(CPIu, on=["State", "Year", "Period"])
    .merge(corePCE, on=["State", "Year", "Period"])
    .merge(PCE, on=["State", "Year", "Period"])
    .merge(Int, on=["State", "Year", "Period"])
    .merge(Assets, on=["State", "Year", "Period"])
    .merge(RPFI, on=["State", "Year", "Period"])
    .merge(Prod, on=["State", "Year", "Period"])
    .merge(Unemp, on=["State", "Year", "Period"])
    .merge(CPI1, on=["State", "Year", "Period"])
    .merge(moredeaths, on=["State", "Year", "Period"], how="left")
    .merge(CDCcasesdeaths, on=["State", "Year", "Period"], how="left")
    .merge(JHUcases, on=["State", "Year", "Period"], how="left")
    .merge(Vax1, on=["State", "Year", "Period"], how="left")
    .merge(Vax2, on=["State", "Year", "Period"], how="left")
    .merge(Distance, on=["State", "Year", "Period"], how="left")
    .merge(Mask, on=["State", "Year", "Period"], how="left")
    .merge(ARP, on=["State", "Year", "Period"], how="left")
    .merge(PPP, on=["State", "Year", "Period"], how="left")
    .merge(CARES, on=["State", "Year", "Period"], how="left")
    .merge(Fedfunds, on=["State", "Year", "Period"])
    .merge(sah, on=["State", "Year", "Period"])
    .merge(cdcsurv, on=["State", "Year", "Period"], how="left")
)

df = df.fillna(0)

nominalValues = df.head(10)
print(nominalValues[["Demand_1"]])
print(nominalValues[["Demand_2"]])
print(nominalValues[["Demand_3"]])
print(nominalValues[["Demand_4"]])
print(nominalValues[["Demand_5"]])
print(nominalValues[["Supply_1"]])
print(nominalValues[["Supply_4"]])

# Adjusting for inflation
print(df[["Demand_1"]])
print(df[["Monetary_3"]])
df["Demand_1"] = df["Demand_1"].div(df["Monetary_3"] / 100)
df["Demand_2"] = df["Demand_2"].div(df["Monetary_3"] / 100)
df["Demand_3"] = df["Demand_3"].div(df["Monetary_3"] / 100)
df["Demand_4"] = df["Demand_4"].div(df["Monetary_3"] / 100)
df["Demand_5"] = df["Demand_5"].div(df["Monetary_3"] / 100)
df["Supply_1"] = df["Supply_1"].div(df["Monetary_3"] / 100)
df["Supply_6"] = df["Supply_6"].div(df["Monetary_3"] / 100)

print(df[["Demand_1"]])

realValues = df.head(10)

print(realValues[["Demand_1"]])
print(realValues[["Demand_2"]])
print(realValues[["Demand_3"]])
print(realValues[["Demand_4"]])
print(realValues[["Demand_5"]])
print(realValues[["Supply_1"]])
print(realValues[["Supply_6"]])

government_fund_dist_unif = {0: 1 / 6, 1: 1 / 6, 2: 1 / 6, 3: 1 / 6, 4: 1 / 6, 5: 1 / 6}
# t = time
# a = value
# r = (1/a)^(1/t)
# government_fund_dist_exp = {0:a(1 â€“ r)t,1:1/6,2:1/6,3:1/6,4:1/6,5:1/6}
print(government_fund_dist_unif)

for i in range(0, len(df)):
    if df.loc[i, "Pandemic_Response_13"] > 0:
        for n in range(0, len(government_fund_dist_unif)):
            df.loc[i + n, "Pandemic_Response_13"] = df.loc[i, "Pandemic_Response_13"] * government_fund_dist_unif[n]
        df.loc[i, "Pandemic_Response_13"] = df.loc[i, "Pandemic_Response_13"] * government_fund_dist_unif[0]
        break

for i in range(0, len(df)):
    if df.loc[i, "Pandemic_Response_14"] > 0:
        for n in range(0, len(government_fund_dist_unif)):
            df.loc[i + n, "Pandemic_Response_14"] = df.loc[i, "Pandemic_Response_14"] * government_fund_dist_unif[n]
        df.loc[i, "Pandemic_Response_14"] = df.loc[i, "Pandemic_Response_14"] * government_fund_dist_unif[0]
        break

for i in range(0, len(df)):
    if df.loc[i, "Pandemic_Response_15"] > 0:
        for n in range(0, len(government_fund_dist_unif)):
            df.loc[i + n, "Pandemic_Response_15"] = df.loc[i, "Pandemic_Response_15"] * government_fund_dist_unif[n]
        df.loc[i, "Pandemic_Response_15"] = df.loc[i, "Pandemic_Response_15"] * government_fund_dist_unif[0]
        break

df["Month"] = pd.to_numeric(df["Period"].apply(lambda x: x[1:]))
df["Day"] = 1
df["Time"] = pd.to_datetime(dict(year=df["Year"], month=df["Month"], day=df["Day"]))
df = df.drop(columns=["Period", "Month", "Year", "Day"])

# Commented out IPython magic to ensure Python compatibility.
# %pip install fastparquet

from fastparquet import write

write("outfile.parq", df)
import shutil

shutil.copy("outfile.parq", "/content/drive/Shareddrives/EB-198 Research/Data/output/")

dfwide = df.loc[df["State"] == "NY"]
dfwide = dfwide[:-12]
# This is where we slice it to be a single state. The trunctation will be removed when data is updated in OCT

"""Here are the states:  AK AL AR AZ CA CO CT DE FL GA HI IA ID IL IN KS KY LA MA MD ME MI MN MO MS MT NC ND NE NH NJ NM NV NY OH OK OR PA RI SC SD TN TX UT VA VT WA WI WV WY"""

# Pivot the table
dfwide = dfwide.pivot(index="Time", columns="State")


def is_constant(column):
    return all(column == column.iloc[0])


# Check each column for constancy
constant_columns = [col for col in dfwide.columns if is_constant(dfwide[col])]

# Print the constant columns
print("Constant columns:", constant_columns)
# Drop the constant columns
dfwide = dfwide.drop(columns=constant_columns)
dfwide = dfwide.fillna(0)

# Render all series normalized and stationary. This will scale them for the post-DFM Synthetic Control Model.
from sklearn.preprocessing import MinMaxScaler
from statsmodels.tsa.stattools import adfuller

scaler = MinMaxScaler()
normalized_df = pd.DataFrame(scaler.fit_transform(dfwide), columns=dfwide.columns) * 100
stationary_df = normalized_df.diff()
stationary_df = stationary_df.fillna(0)

non_stationary_columns = []
for column in stationary_df.columns:
    result = adfuller(stationary_df[column])
    p_value = result[1]
    if p_value > 0.25:
        non_stationary_columns.append(column)


print("Columns that fail the ADF test (non-stationary):", non_stationary_columns)

# to see csv currently in tabular format
# stationary_df.to_csv('stationary.csv', encoding='utf-8', index=False)

# define dictionary, need to call the state being currently used
factors = {}
for State in ["NY"]:
    for n in range(1, 5):
        PH = "Supply_" + str(n) + "_" + str(State)
        factors[PH] = "Global", "Supply"
    for n in range(5, 8):
        PH = "Supply_" + str(n) + "_" + str(State)
        factors[PH] = "Global", "Uncat"
    for n in range(1, 6):
        PH = "Demand_" + str(n) + "_" + str(State)
        factors[PH] = "Global", "Demand"
    for n in range(6, 8):
        PH = "Demand_" + str(n) + "_" + str(State)
        factors[PH] = "Global", "Uncat"
    for n in range(1, 13):
        PH = "Pandemic_" + str(n) + "_" + str(State)
        factors[PH] = "Global", "Pandemic"
    for n in range(1, 16):
        PH = "Pandemic_Response_" + str(n) + "_" + str(State)
        factors[PH] = "Global", "Response"
    for n in range(1, 5):
        PH = "Monetary_" + str(n) + "_" + str(State)
        factors[PH] = "Global", "Inflation"
    for n in range(5, 12):
        PH = "Monetary_" + str(n) + "_" + str(State)
        factors[PH] = "Global", "Uncat"
    PH = "proportion_vax2" + "_" + str(State)
    factors[PH] = "Global", "Pandemic"
    PH = "Proportion" + "_" + str(State)
    factors[PH] = "Global", "Pandemic"

print(factors)


def flatten_multiindex_columns(df):
    cols = df.columns.to_flat_index()
    new_cols = []
    for i, col in enumerate(cols):
        new_col = "_".join(map(str, col))
        new_cols.append(new_col)
    new_df = pd.DataFrame(df.values, columns=new_cols)
    return new_df


flattened_df = flatten_multiindex_columns(stationary_df)

df_filled = flattened_df.fillna(0)
# print(df_filled)
# print(factors)

import statsmodels.api as sm

factor_multiplicities = {"Global": 2}
model = sm.tsa.DynamicFactorMQ(df_filled, factors=factors, factor_multiplicities=factor_multiplicities)

model.summary()

results = model.fit(disp=10)

results.summary()

# Commented out IPython magic to ensure Python compatibility.
# %pip install fastparquet
df.to_parquet("dfwide.parquet.gzip", compression="gzip")
# cp -r 'dfwide.parquet.gzip' '/content/drive/Shareddrives/EB-198 Research/Data/output/'
with open("/content/drive/Shareddrives/EB-198 Research/Data/output/factors.json", "w") as outfile:
    json.dump(factors, outfile)

"""The DFM will assign observables to the latent variables to be estimated by forcing the values of the weighting matrix to be zero when a core variable interacts with a core observation from another assignment. There are 5 core classes, Supply, Demand, Pandemic, Pandemic Response and Monetary.

Then have 2 non core that everything is determined by.

https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.dynamic_factor_mq.DynamicFactorMQ.html

Is the command to estimate the model.
"""
